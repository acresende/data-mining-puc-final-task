{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------- Bibliotecas Utilizadas ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------- Read Files ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data with `read_csv()`\n",
    "horsesDataSet = pd.read_csv('horse.csv', header=0, delimiter=',')\n",
    "\n",
    "horsesDataSetTest = pd.read_csv(\"horseTest.csv\", header=0, delimiter=',')\n",
    "\n",
    "#description of dataSet\n",
    "descriptionHorsesDataSet = horsesDataSet.describe(include='all')\n",
    "descriptionHorsesDataSetTest = horsesDataSetTest.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Base de Trieno (Arquivo horse.csv)\\n')\n",
    "descriptionHorsesDataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nBase de Teste (Arquivo horseTest.csv)\\n')\n",
    "descriptionHorsesDataSetTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------- Exploratory analysis ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 5 and last 5 entries in dataSet\n",
    "firstRowsDataSet = horsesDataSet.head(5)\n",
    "lastRowsDataSet = horsesDataSet.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstRowsDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastRowsDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling data\n",
    "# Take a sample of 5\n",
    "horsesDataSetSample = horsesDataSet.sample(5)\n",
    "horsesDataSetSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulls\n",
    "result = pd.isnull(horsesDataSet)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------- Pre processing ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each attribute and define the percentage of missing values\n",
    "# populate array with zeros with column dimensions of dataset\n",
    "qtd_nan = [0 for x in range(horsesDataSet.shape[1])]\n",
    "\n",
    "# populate array with zeros with column dimensions of dataset\n",
    "qtd_total = [0 for x in range(horsesDataSet.shape[1])]\n",
    "\n",
    "i = 0\n",
    "while i < horsesDataSet.shape[1]:\n",
    "    # get array of boolean describing each line as null or not for i attribute\n",
    "    attributeLinesIsNA = pd.isna(horsesDataSet.iloc[:, i])\n",
    "\n",
    "    # get current attribute label name\n",
    "    currentAttributeLabel = list(horsesDataSet)[i]\n",
    "\n",
    "    qtd_nan[i] = horsesDataSet.loc[attributeLinesIsNA, currentAttributeLabel].shape[0]\n",
    "    qtd_total[i] = horsesDataSet.loc[:, currentAttributeLabel].shape[0]\n",
    "    i = i+1\n",
    "    \n",
    "percentageArray = np.divide(qtd_nan, qtd_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping atributes\n",
    "threshold = 0.5\n",
    "PreProcessedHorseDataSet = horsesDataSet\n",
    "PreProcessedHorseDataSetTest = horsesDataSetTest\n",
    "i = 0\n",
    "while i < horsesDataSet.shape[1]:\n",
    "    if percentageArray[i] > threshold:\n",
    "        # get current attribute label name\n",
    "        currentAttributeLabel = list(horsesDataSet)[i]\n",
    "        \n",
    "        # drop attribute column if na values > threshold\n",
    "        PreProcessedHorseDataSet = PreProcessedHorseDataSet.drop(columns=currentAttributeLabel)\n",
    "        \n",
    "        #drop from test\n",
    "        PreProcessedHorseDataSetTest = PreProcessedHorseDataSetTest.drop(columns=currentAttributeLabel)\n",
    "        \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill remaining lines with mean values (only numerical)\n",
    "PreProcessedHorseDataSet = PreProcessedHorseDataSet.fillna(horsesDataSet.mean())\n",
    "#PreProcessedHorseDataSetTest = PreProcessedHorseDataSetTest.fillna(horsesDataSetTest.mean())\n",
    "\n",
    "# Show Statistics of DataSet\n",
    "StatisticsPreProcessedHorseDataSet = PreProcessedHorseDataSet.describe(include='all')\n",
    "\n",
    "# Altering Categorical missing values to Mode Value (value that appear the most often)\n",
    "i = 0\n",
    "while i < PreProcessedHorseDataSet.shape[1]:\n",
    "    # return the most frequent value (first index because mode() returns a DataFrame)\n",
    "    attributeMode = PreProcessedHorseDataSet.mode().iloc[0, i]\n",
    "    currentAttributeLabel = list(PreProcessedHorseDataSet)[i]\n",
    "    PreProcessedHorseDataSet[currentAttributeLabel] = PreProcessedHorseDataSet[currentAttributeLabel].fillna(attributeMode)\n",
    "    i = i+1\n",
    "\n",
    "# Altering missing values [DATASET TEST]\n",
    "#Saving values from train to insret in TEST with variable v\n",
    "v = [0 for x in range(horsesDataSet.shape[1])]\n",
    "i=0\n",
    "while i < PreProcessedHorseDataSet.shape[1]:\n",
    "    if PreProcessedHorseDataSet.dtypes[i] == 'O':\n",
    "        v[i] = PreProcessedHorseDataSet.mode().iloc[0, i]\n",
    "    else:\n",
    "        v[i] = PreProcessedHorseDataSet.iloc[0, i].mean()\n",
    "    \n",
    "    currentAttributeLabel = list(PreProcessedHorseDataSetTest)[i]\n",
    "    PreProcessedHorseDataSetTest[currentAttributeLabel] = PreProcessedHorseDataSetTest[currentAttributeLabel].fillna(v[i])\n",
    "    i = i+1\n",
    "\n",
    "#i = 0\n",
    "#while i < PreProcessedHorseDataSetTest.shape[1]:\n",
    "#    attributeMode = PreProcessedHorseDataSetTest.mode().iloc[0, i]\n",
    "#    currentAttributeLabel = list(PreProcessedHorseDataSetTest)[i]\n",
    "#    PreProcessedHorseDataSetTest[currentAttributeLabel] = PreProcessedHorseDataSetTest[currentAttributeLabel].fillna(attributeMode)\n",
    "#    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical attribute binarization\n",
    "categoricalHorseDataSet = PreProcessedHorseDataSet.select_dtypes(include='object')\n",
    "categoricalHorseDataSet = categoricalHorseDataSet.drop('outcome', axis=1)\n",
    "categoricalHorseDataSetDummy = pd.get_dummies(categoricalHorseDataSet)\n",
    "PreProcessedHorseDataSet = pd.concat([categoricalHorseDataSetDummy, PreProcessedHorseDataSet.loc[:, 'outcome']], axis=1)\n",
    "\n",
    "# categorical attribute binarization [DATASET TEST]\n",
    "categoricalHorseDataSetTest = PreProcessedHorseDataSetTest.select_dtypes(include='object')\n",
    "categoricalHorseDataSetTest = categoricalHorseDataSetTest.drop('outcome', axis=1)\n",
    "categoricalHorseDataSetDummy = pd.get_dummies(categoricalHorseDataSetTest)\n",
    "PreProcessedHorseDataSetTest = pd.concat([categoricalHorseDataSetDummy, PreProcessedHorseDataSetTest.loc[:, 'outcome']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change values from euthanized to died\n",
    "AttributesHorseDataSet = PreProcessedHorseDataSet.drop('outcome', axis=1)\n",
    "TargetHorseDataSet = PreProcessedHorseDataSet.loc[:, 'outcome']\n",
    "\n",
    "# mapping 'euthanized' values to 'died' to tune fitting\n",
    "TargetHorseDataSet = TargetHorseDataSet.map(lambda x: 'died' if x == 'euthanized' else x)\n",
    "\n",
    "PreProcessedHorseDataSet = pd.concat([AttributesHorseDataSet, TargetHorseDataSet], axis=1)\n",
    "\n",
    "# Change values from euthanized to died [DATASET TEST]\n",
    "AttributesHorseDataSetTest = PreProcessedHorseDataSetTest.drop('outcome', axis=1)\n",
    "TargetHorseDataSetTest = PreProcessedHorseDataSetTest.loc[:, 'outcome']\n",
    "\n",
    "# mapping 'euthanized' values to 'died' to tune fitting\n",
    "TargetHorseDataSetTest = TargetHorseDataSetTest.map(lambda x: 'died' if x == 'euthanized' else x)\n",
    "PreProcessedHorseDataSetTest = pd.concat([AttributesHorseDataSetTest, TargetHorseDataSetTest], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo objetos para categÃ³ricos\n",
    "i= 0\n",
    "while i < PreProcessedHorseDataSet.shape[1]:\n",
    "    if PreProcessedHorseDataSet[list(PreProcessedHorseDataSet)[i]].dtypes == 'O': \n",
    "        PreProcessedHorseDataSet[list(PreProcessedHorseDataSet)[i]] = PreProcessedHorseDataSet[list(PreProcessedHorseDataSet)[i]].astype('category')\n",
    "    i = i+1\n",
    "\n",
    "i= 0\n",
    "while i < PreProcessedHorseDataSetTest.shape[1]:\n",
    "    if PreProcessedHorseDataSetTest[list(PreProcessedHorseDataSetTest)[i]].dtypes == 'O': \n",
    "        PreProcessedHorseDataSetTest[list(PreProcessedHorseDataSetTest)[i]] = PreProcessedHorseDataSetTest[list(PreProcessedHorseDataSetTest)[i]].astype('category')\n",
    "    i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nBase de treino jÃ¡ preparada para utilizar os modelos\\n')\n",
    "PreProcessedHorseDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nBase de teste jÃ¡ preparada para testar os modelos\\n')\n",
    "PreProcessedHorseDataSetTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------- Applying Models ---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(PreProcessedHorseDataSet.drop('outcome', axis = 1), PreProcessedHorseDataSet['outcome'], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanho = 100\n",
    "maior_knn = 0\n",
    "pos = 0\n",
    "vet =[]\n",
    "for i in range(1,tamanho):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(x_train, y_train)\n",
    "    result = knn.score(x_test, y_test)\n",
    "    vet.append(result)\n",
    "    if result > maior_knn:\n",
    "        maior_knn = result\n",
    "        pos = i\n",
    "        TargetHorseDataSet_prediction = knn.predict(x_test)\n",
    "        TargetHorseDataSet_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = pos)\n",
    "knn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(PreProcessedHorseDataSetTest.drop('outcome', axis = 1), PreProcessedHorseDataSetTest.outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TargetHorseDataSet_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate metrics\n",
    "\n",
    "accuracyScore = metrics.accuracy_score(TargetHorseDataSet_test, TargetHorseDataSet_prediction)\n",
    "print('Accuracy: ' || accuracyScore)\n",
    "recallScore = metrics.recall_score(TargetHorseDataSet_test, TargetHorseDataSet_prediction, average=None)\n",
    "print(recallScore)\n",
    "kappaScore = metrics.cohen_kappa_score(TargetHorseDataSet_test, TargetHorseDataSet_prediction)\n",
    "print(kappaScore)\n",
    "\n",
    "#TargetHorseDataSet_test = labelEncoder.inverse_transform(TargetHorseDataSet_test)\n",
    "#TargetHorseDataSet_prediction = labelEncoder.inverse_transform(TargetHorseDataSet_prediction)\n",
    "\n",
    "confusionMatrix = pd.DataFrame(\n",
    "    metrics.confusion_matrix(TargetHorseDataSet_test, TargetHorseDataSet_prediction),\n",
    "    columns=['Predicted Died', 'Predicted Lived'],\n",
    "    index=['True Died', 'True Lived']\n",
    ")\n",
    "\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN COM REGRESSÃO LOGÃSTICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lr = 0\n",
    "melhor_qty = 0\n",
    "resultados = []\n",
    "\n",
    "for j in range(1,len(PreProcessedHorseDataSet)):\n",
    "    model = LogisticRegression()\n",
    "    feature_qty = j\n",
    "    dataset = PreProcessedHorseDataSet\n",
    "    rfe = RFE(model, feature_qty)\n",
    "    rfe = rfe.fit(dataset.drop('outcome', axis = 1), dataset.outcome.values)\n",
    "\n",
    "    col = []\n",
    "    for i in range(0,rfe.support_.size):\n",
    "        if rfe.support_[i] == True:\n",
    "            col.append(i)\n",
    "\n",
    "    dataset_after_rfe = dataset[dataset.columns[col]]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(dataset_after_rfe, dataset['outcome'], random_state = 0)\n",
    "    \n",
    "    tamanho = 100\n",
    "    maior_knn = 0\n",
    "    pos = 0\n",
    "    vet =[]\n",
    "    for i in range(1,tamanho):\n",
    "        knn = KNeighborsClassifier(n_neighbors = i)\n",
    "        knn.fit(x_train, y_train)\n",
    "        result = knn.score(x_test, y_test)\n",
    "        vet.append(result)\n",
    "        if result > maior_knn:\n",
    "            maior_knn = result\n",
    "            pos = i\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors = pos)\n",
    "    knn.fit(x_train, y_train)\n",
    "    \n",
    "    TESTE = PreProcessedHorseDataSetTest[list(dataset[dataset.columns[col]].columns)]\n",
    "    \n",
    "    h = knn.score(TESTE, PreProcessedHorseDataSetTest.outcome)\n",
    "    resultados.append(h)\n",
    "    if h > result_lr:\n",
    "        result_lr = h\n",
    "        melhor_qty = j\n",
    "        TargetHorseDataSet_prediction = knn.predict(x_test)\n",
    "        TargetHorseDataSet_test = y_test\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_lr, melhor_qty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TargetHorseDataSet_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate metrics\n",
    "\n",
    "accuracyScore = metrics.accuracy_score(TargetHorseDataSet_test, TargetHorseDataSet_prediction)\n",
    "print(accuracyScore)\n",
    "recallScore = metrics.recall_score(TargetHorseDataSet_test, TargetHorseDataSet_prediction, average=None)\n",
    "print(recallScore)\n",
    "kappaScore = metrics.cohen_kappa_score(TargetHorseDataSet_test, TargetHorseDataSet_prediction)\n",
    "print(kappaScore)\n",
    "\n",
    "#TargetHorseDataSet_test = labelEncoder.inverse_transform(TargetHorseDataSet_test)\n",
    "#TargetHorseDataSet_prediction = labelEncoder.inverse_transform(TargetHorseDataSet_prediction)\n",
    "\n",
    "confusionMatrix = pd.DataFrame(\n",
    "    metrics.confusion_matrix(TargetHorseDataSet_test, TargetHorseDataSet_prediction),\n",
    "    columns=['Predicted Died', 'Predicted Lived'],\n",
    "    index=['True Died', 'True Lived']\n",
    ")\n",
    "\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoder\n",
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "labelEncoder.fit(TargetHorseDataSet.values)\n",
    "TargetHorseEncodedArray = labelEncoder.transform(TargetHorseDataSet.values)\n",
    "TargetHorseEncodedDataSet = pd.DataFrame(TargetHorseEncodedArray, columns=['outcome'])\n",
    "\n",
    "# split train and test data and target\n",
    "AttributesHorseDataSet_train, AttributesHorseDataSet_test, TargetHorseDataSet_train, TargetHorseDataSet_test = train_test_split(AttributesHorseDataSet, TargetHorseEncodedDataSet, random_state=1)\n",
    "\n",
    "# initialize model parameters\n",
    "decisionTreeModel = tree.DecisionTreeClassifier()\n",
    "\n",
    "# fit model using training data\n",
    "decisionTreeModel.fit(AttributesHorseDataSet_train, TargetHorseDataSet_train)\n",
    "\n",
    "# predict our test data using fitted model\n",
    "TargetHorseDataSet_prediction = decisionTreeModel.predict(AttributesHorseDataSet_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TargetHorseDataSet_test = labelEncoder.inverse_transform(TargetHorseDataSet_test)\n",
    "TargetHorseDataSet_prediction = labelEncoder.inverse_transform(TargetHorseDataSet_prediction)\n",
    "TargetHorseDataSet_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate metrics\n",
    "accuracyScore = metrics.accuracy_score(TargetHorseDataSet_test, TargetHorseDataSet_prediction)\n",
    "print(accuracyScore)\n",
    "recallScore = metrics.recall_score(TargetHorseDataSet_test, TargetHorseDataSet_prediction, average=None)\n",
    "print(recallScore)\n",
    "kappaScore = metrics.cohen_kappa_score(TargetHorseDataSet_test, TargetHorseDataSet_prediction)\n",
    "print(kappaScore)\n",
    "\n",
    "confusionMatrix = pd.DataFrame(\n",
    "    metrics.confusion_matrix(TargetHorseDataSet_test, TargetHorseDataSet_prediction),\n",
    "    columns=['Predicted Lived', 'Predicted Died'],\n",
    "    index=['True Lived', 'True Died']\n",
    ")\n",
    "print(confusionMatrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE COM REGRESSÃO LOGÃSTICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lr = 0\n",
    "melhor_qty = 0\n",
    "resultados = []\n",
    "\n",
    "for j in range(1,len(PreProcessedHorseDataSet)):\n",
    "    model = LogisticRegression()\n",
    "    feature_qty = j\n",
    "    dataset = PreProcessedHorseDataSet\n",
    "    rfe = RFE(model, feature_qty)\n",
    "    rfe = rfe.fit(dataset.drop('outcome', axis = 1), dataset.outcome.values)\n",
    "\n",
    "    col = []\n",
    "    for i in range(0,rfe.support_.size):\n",
    "        if rfe.support_[i] == True:\n",
    "            col.append(i)\n",
    "\n",
    "    dataset_after_rfe = dataset[dataset.columns[col]]\n",
    "    \n",
    "    # label encoder\n",
    "    labelEncoder = preprocessing.LabelEncoder()\n",
    "    labelEncoder.fit(TargetHorseDataSet.values)\n",
    "    TargetHorseEncodedArray = labelEncoder.transform(TargetHorseDataSet.values)\n",
    "    TargetHorseEncodedDataSet = pd.DataFrame(TargetHorseEncodedArray, columns=['outcome'])\n",
    "\n",
    "    # split train and test data and target\n",
    "    AttributesHorseDataSet_train, AttributesHorseDataSet_test, TargetHorseDataSet_train, TargetHorseDataSet_test = train_test_split(dataset_after_rfe, TargetHorseEncodedDataSet, random_state=1)\n",
    "\n",
    "    # initialize model parameters\n",
    "    decisionTreeModel = tree.DecisionTreeClassifier()\n",
    "\n",
    "    # fit model using training data\n",
    "    decisionTreeModel.fit(AttributesHorseDataSet_train, TargetHorseDataSet_train)\n",
    "\n",
    "    # predict our test data using fitted model\n",
    "    TargetHorseDataSet_prediction = decisionTreeModel.predict(AttributesHorseDataSet_test)\n",
    "    \n",
    "    \n",
    "    accuracyScore = metrics.accuracy_score(TargetHorseDataSet_test, TargetHorseDataSet_prediction)\n",
    "    \n",
    "    if accuracyScore > result_lr:\n",
    "        result_lr = accuracyScore\n",
    "        melhor_qty = j\n",
    "        TargetHorseDataSet_prediction_Store = decisionTreeModel.predict(AttributesHorseDataSet_test)\n",
    "        TargetHorseDataSet_test_Store = TargetHorseDataSet_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TargetHorseDataSet_test = labelEncoder.inverse_transform(TargetHorseDataSet_test)\n",
    "TargetHorseDataSet_prediction = labelEncoder.inverse_transform(TargetHorseDataSet_prediction)\n",
    "TargetHorseDataSet_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyScore = metrics.accuracy_score(TargetHorseDataSet_test_Store, TargetHorseDataSet_prediction_Store)\n",
    "print(accuracyScore)\n",
    "recallScore = metrics.recall_score(TargetHorseDataSet_test_Store, TargetHorseDataSet_prediction_Store, average=None)\n",
    "print(recallScore)\n",
    "kappaScore = metrics.cohen_kappa_score(TargetHorseDataSet_test_Store, TargetHorseDataSet_prediction_Store)\n",
    "print(kappaScore)\n",
    "\n",
    "#TargetHorseDataSet_test = labelEncoder.inverse_transform(TargetHorseDataSet_test)\n",
    "#TargetHorseDataSet_prediction = labelEncoder.inverse_transform(TargetHorseDataSet_prediction)\n",
    "\n",
    "confusionMatrix = pd.DataFrame(\n",
    "    metrics.confusion_matrix(TargetHorseDataSet_test_Store, TargetHorseDataSet_prediction_Store),\n",
    "    columns=['Predicted Died', 'Predicted Lived'],\n",
    "    index=['True Died', 'True Lived']\n",
    ")\n",
    "\n",
    "print(confusionMatrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
